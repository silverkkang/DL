{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1FVa9MMILGSecZb6uuTSqEYOg6ypY615P",
      "authorship_tag": "ABX9TyPjGkAYP7WUWiK2Usdvk5xJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverkkang/DL/blob/main/bertopic251118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_BPdf2PVtZQ",
        "outputId": "f4a98e5b-0bab-49ff-b5fe-5978c9d0a11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn\n",
        "!pip install transformers torch\n",
        "!pip install scikit-learn matplotlib\n",
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "aJVa8OifWvF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/drive/MyDrive/Colab Notebooks/á„Œá…µá†«á„á…¡ á„á…¬á„Œá…©á†¼á„Œá…©á†¼á„’á…¡á†¸á„‡á…©á†«.csv'\n",
        "df = pd.read_csv(file_name, engine='python', on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "h1QjZlzxWxeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JmfR8A3W15o",
        "outputId": "915480b0-69cb-4842-e560-cc53e0cfedc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                 title  \\\n",
            "0           0          ì†ŒìŒìˆœ ëŒì¶œ ë˜ì–´ ë¶ˆí¸í•¨ ê²ªì–´ë³´ì‹  ë¶„ ê³„ì‹¤ê¹Œìš”..?   \n",
            "1           1                        ì„ë¶€ë³µ ë°”ì§€ ë„ì™€ì£¼ì„¸ìš” ã…    \n",
            "2           2  ëª¨ìœ ìˆ˜ìœ  ë¶„ìœ  ìƒê´€ ì—†ì´ ìˆ˜ìœ ì¿ ì…˜ì€ ì“°ë‚˜ìš”? ë‘˜ì¤‘ ë­ê°€ í¸í• ê¹Œìš”!   \n",
            "\n",
            "                                            contents  \\\n",
            "0  ì§€ê¸ˆ ì„ì‹  5ì£¼ì°¨ì˜ˆìš” ~ì˜ˆì „ì— ì‚°ë¶€ì¸ê³¼ì—ì„œ ê²€ì‚¬í•˜ë©´ì„œì†ŒìŒìˆœì´ ë¹„ëŒ€ì¹­ì´ë„¤ ë¼ê³  ì›ì¥ë‹˜...   \n",
            "1  ìŠ¤í† í¼í˜• í•˜ë‚˜ì‚¬ë´¤ëŠ”ë° ì¡°ì´ë©´ ì¡°ì¸ê³³ì´ ë„ˆë¬´ ë¶ˆí¸í•˜ê³ ì•ˆì¡°ì´ë©´ í˜ëŸ¬ë‚´ë¦¬ê³  ã…œì›í”¼ìŠ¤ ì‚¬ê¸´...   \n",
            "2         ê°€ê²© ìƒê´€ ì—†ì´ ë­ê°€ ë‚˜ì•„ë³´ì´ë‚˜ìš”?ìˆ˜ìœ  ì–´ë–»ê²Œ í• ì§€ ëª¨ë¥´ê² ëŠ”ë° ì˜ì“¸ê¹Œìš”,,?   \n",
            "\n",
            "                                                 url        date  nouns  \n",
            "0  https://cafe.naver.com/f-e/cafes/10094499/arti...  2025-11-15    NaN  \n",
            "1  https://cafe.naver.com/f-e/cafes/10094499/arti...  2025-11-15    NaN  \n",
            "2  https://cafe.naver.com/f-e/cafes/10094499/arti...  2025-11-15    NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1. ì „ì²˜ë¦¬\n"
      ],
      "metadata": {
        "id": "oGflg7Gutpui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_COLUMN = 'contents'\n",
        "original_count = len(df)"
      ],
      "metadata": {
        "id": "y8-ik6R1XTch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 ê²°ì¸¡ì¹˜ ì œê±°\n",
        "df = df.dropna(subset=[TEXT_COLUMN])\n",
        "print(f\"1ï¸âƒ£  ê²°ì¸¡ì¹˜ ì œê±°: {original_count - len(df):,}ê°œ â†’ {len(df):,}ê°œ ë‚¨ìŒ\")\n",
        "\n",
        "# ë¹ˆ ë¬¸ìì—´ ì œê±°\n",
        "before_count = len(df)\n",
        "df = df[df[TEXT_COLUMN].str.strip() != '']\n",
        "print(f\"2ï¸âƒ£  ë¹ˆ ë¬¸ìì—´ ì œê±°: {before_count - len(df):,}ê°œ â†’ {len(df):,}ê°œ ë‚¨ìŒ\")\n",
        "\n",
        "# ì •ê·œí‘œí˜„ì‹ ì •ë¦¬\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # HTML\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)  # URL\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Email\n",
        "    text = re.sub(r'([!?])\\1{2,}', r'\\1', text)  # íŠ¹ìˆ˜ë¬¸ì\n",
        "    text = re.sub(r'(\\.)\\1{3,}', r'\\1\\1\\1', text)\n",
        "    text = re.sub(r'([ã…‹ã…ã… ã…œã…¡])\\1{2,}', r'\\1\\1', text)  # í•œê¸€ ììŒ\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)\n",
        "df = df[df[TEXT_COLUMN] != '']\n",
        "\n",
        "# ì¤‘ë³µ ì œê±°\n",
        "before_count = len(df)\n",
        "df = df.drop_duplicates(subset=[TEXT_COLUMN], keep='first')\n",
        "\n",
        "# ìµœì†Œ ê¸¸ì´ í•„í„°ë§\n",
        "MIN_LENGTH = 5\n",
        "before_count = len(df)\n",
        "df = df[df[TEXT_COLUMN].str.len() >= MIN_LENGTH]\n",
        "\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6OfvRutXXgE",
        "outputId": "cfcd40f9-7e8e-4125-e64f-28cf686d7fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1ï¸âƒ£  ê²°ì¸¡ì¹˜ ì œê±°: 33ê°œ â†’ 90,160ê°œ ë‚¨ìŒ\n",
            "2ï¸âƒ£  ë¹ˆ ë¬¸ìì—´ ì œê±°: 1ê°œ â†’ 90,159ê°œ ë‚¨ìŒ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"\\nì „ì²˜ë¦¬ ì™„ë£Œ: {original_count:,}ê°œ â†’ {len(df):,}ê°œ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkopnEFxXdR8",
        "outputId": "3cbb5167-0157-43e1-e26f-628831865bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ì „ì²˜ë¦¬ ì™„ë£Œ: 90,193ê°œ â†’ 90,104ê°œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
        "df['date'] = pd.to_datetime(df['date'])"
      ],
      "metadata": {
        "id": "rBDIu_70aA0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—°ë„-ë¶„ê¸° ì»¬ëŸ¼ ìƒì„± (ì˜ˆ: '2022-Q3', '2025-Q4')\n",
        "df['year_quarter'] = df['date'].dt.year.astype(str) + '-Q' + df['date'].dt.quarter.astype(str)"
      ],
      "metadata": {
        "id": "nxrgpuDJaArC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¶„ê¸°ë³„ ë¶„í¬ í™•ì¸\n",
        "print(\"\\në¶„ê¸°ë³„ ë°ì´í„° ë¶„í¬:\")\n",
        "print(\"-\" * 50)\n",
        "quarter_counts = df['year_quarter'].value_counts().sort_index()\n",
        "for quarter, count in quarter_counts.items():\n",
        "    percentage = count / len(df) * 100\n",
        "    bar = \"â–ˆ\" * int(percentage / 2)\n",
        "    print(f\"{quarter}  |  {count:6,}ê°œ  ({percentage:5.1f}%)  {bar}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"ì´ {len(quarter_counts)}ê°œ ë¶„ê¸°\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64eGE2f4aGek",
        "outputId": "eb693175-663a-4f01-f3ba-fdaff59e884a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ë¶„ê¸°ë³„ ë°ì´í„° ë¶„í¬:\n",
            "--------------------------------------------------\n",
            "2021.0-Q1.0  |      11ê°œ  (  0.0%)  \n",
            "2021.0-Q2.0  |     189ê°œ  (  0.2%)  \n",
            "2021.0-Q3.0  |     286ê°œ  (  0.3%)  \n",
            "2021.0-Q4.0  |     353ê°œ  (  0.4%)  \n",
            "2022.0-Q1.0  |     475ê°œ  (  0.5%)  \n",
            "2022.0-Q2.0  |     621ê°œ  (  0.7%)  \n",
            "2022.0-Q3.0  |     807ê°œ  (  0.9%)  \n",
            "2022.0-Q4.0  |     896ê°œ  (  1.0%)  \n",
            "2023.0-Q1.0  |     973ê°œ  (  1.1%)  \n",
            "2023.0-Q2.0  |     895ê°œ  (  1.0%)  \n",
            "2023.0-Q3.0  |   1,093ê°œ  (  1.2%)  \n",
            "2023.0-Q4.0  |     887ê°œ  (  1.0%)  \n",
            "2024.0-Q1.0  |   1,024ê°œ  (  1.1%)  \n",
            "2024.0-Q2.0  |   1,373ê°œ  (  1.5%)  \n",
            "2024.0-Q3.0  |   2,098ê°œ  (  2.3%)  â–ˆ\n",
            "2024.0-Q4.0  |   1,915ê°œ  (  2.1%)  â–ˆ\n",
            "2025.0-Q1.0  |   2,374ê°œ  (  2.6%)  â–ˆ\n",
            "2025.0-Q2.0  |   4,291ê°œ  (  4.8%)  â–ˆâ–ˆ\n",
            "2025.0-Q3.0  |   8,075ê°œ  (  9.0%)  â–ˆâ–ˆâ–ˆâ–ˆ\n",
            "2025.0-Q4.0  |   3,916ê°œ  (  4.3%)  â–ˆâ–ˆ\n",
            "nan-Qnan  |  57,552ê°œ  ( 63.9%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "--------------------------------------------------\n",
            "ì´ 21ê°œ ë¶„ê¸°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1ì¸µí™” ê¸°ì¤€ ì„¤ì •\n",
        "STRATIFY_COLUMN = None\n",
        "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNmmydp2X4kH",
        "outputId": "aa42f6d2-3977-48fb-f4c6-e091ffae7145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: ['Unnamed: 0', 'title', 'contents', 'url', 'date', 'nouns']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_SIZE = 10000\n",
        "\n",
        "if len(df) > SAMPLE_SIZE:\n",
        "    print(f\"ì „ì²˜ë¦¬ í›„ ë°ì´í„°: {len(df):,}ê°œ\")\n",
        "    print(f\"ìƒ˜í”Œë§ ëª©í‘œ: {SAMPLE_SIZE:,}ê°œ\")\n",
        "\n",
        "    # ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚°\n",
        "    sample_ratio = SAMPLE_SIZE / len(df)\n",
        "\n",
        "    try:\n",
        "        # ë¶„ê¸°ë³„ ì¸µí™” ìƒ˜í”Œë§\n",
        "        df_sampled, _ = train_test_split(\n",
        "            df,\n",
        "            train_size=sample_ratio,\n",
        "            stratify=df['year_quarter'],\n",
        "            random_state=42\n",
        "        )\n",
        "        df_sampled = df_sampled.sort_index().reset_index(drop=True)\n",
        "\n",
        "        print(f\"\\nâœ… ì¸µí™” ìƒ˜í”Œë§ ì™„ë£Œ!\")\n",
        "        print(f\"   - ì „ì²˜ë¦¬ í›„: {len(df):,}ê°œ\")\n",
        "        print(f\"   - ìƒ˜í”Œ ì¶”ì¶œ: {len(df_sampled):,}ê°œ\")\n",
        "        print(f\"   - ìƒ˜í”Œë§ ë¹„ìœ¨: {sample_ratio*100:.1f}%\")\n",
        "\n",
        "        # ìƒ˜í”Œ í›„ ë¶„ê¸°ë³„ ë¶„í¬ í™•ì¸\n",
        "        print(\"\\nìƒ˜í”Œ í›„ ë¶„ê¸°ë³„ ë¶„í¬:\")\n",
        "\n",
        "        sampled_quarter_counts = df_sampled['year_quarter'].value_counts().sort_index()\n",
        "        for quarter in quarter_counts.index:\n",
        "            original = quarter_counts[quarter]\n",
        "            sampled = sampled_quarter_counts.get(quarter, 0)\n",
        "            ratio = sampled / original * 100 if original > 0 else 0\n",
        "            print(f\"{quarter}  |  {original:6,}ê°œ â†’ {sampled:5,}ê°œ  ({ratio:5.1f}%)\")\n",
        "\n",
        "\n",
        "        # ë¹„ìœ¨ì´ ìœ ì§€ë˜ëŠ”ì§€ ê²€ì¦\n",
        "        print(\"\\nì¸µí™” ì¶”ì¶œ ê²€ì¦:\")\n",
        "        print(f\"   ì „ì²´ ìƒ˜í”Œë§ ë¹„ìœ¨: {sample_ratio*100:.2f}%\")\n",
        "        print(f\"   ê° ë¶„ê¸° ìƒ˜í”Œë§ ë¹„ìœ¨: ì•½ {sample_ratio*100:.2f}% (ê· ì¼)\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nâš ï¸  ì¸µí™” ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
        "        print(\"   â†’ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´\")\n",
        "        df_sampled = df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "else:\n",
        "    print(f\"âš ï¸  ë°ì´í„°ê°€ {SAMPLE_SIZE:,}ê°œ ì´í•˜ì…ë‹ˆë‹¤.\")\n",
        "    print(f\"   ì „ì²´ {len(df):,}ê°œ ì‚¬ìš©\")\n",
        "    df_sampled = df.copy()\n"
      ],
      "metadata": {
        "id": "rh1bLSt3YSah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ“ ìƒ˜í”Œë§ í›„ ë°ì´í„° í™•ì¸ (5ê°œ):\")\n",
        "for i, row in df_sampled.head(5).iterrows():\n",
        "    text = row[TEXT_COLUMN]\n",
        "    quarter = row['year_quarter']\n",
        "    print(f\"{i+1}. [{quarter}] {text[:80]}{'...' if len(text) > 80 else ''}\")\n",
        "\n",
        "texts = df_sampled[TEXT_COLUMN].tolist()\n",
        "print(f\"\\nìµœì¢… í•™ìŠµ ë°ì´í„°: {len(texts):,}ê°œ\")"
      ],
      "metadata": {
        "id": "RJ-HIj2TqMMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_data_step1 = {\n",
        "    'df_original': df,  # ì „ì²˜ë¦¬ í›„ ì „ì²´\n",
        "    'df_sampled': df_sampled,  # ìƒ˜í”Œë§ëœ ë°ì´í„°\n",
        "    'TEXT_COLUMN': TEXT_COLUMN,\n",
        "    'stratify_column': 'year_quarter',\n",
        "    'quarter_distribution_original': quarter_counts.to_dict(),\n",
        "    'quarter_distribution_sampled': df_sampled['year_quarter'].value_counts().to_dict(),\n",
        "    'original_count': original_count,\n",
        "    'preprocessed_count': len(df),\n",
        "    'sample_count': len(df_sampled),\n",
        "    'sampling_ratio': SAMPLE_SIZE / len(df) if len(df) > SAMPLE_SIZE else 1.0,\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open('step1_preprocessing_result.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data_step1, f)\n",
        "\n",
        "file_size = os.path.getsize('step1_preprocessing_result.pkl') / (1024**2)\n",
        "print(f\"ì €ì¥ ì™„ë£Œ: step1_preprocessing_result.pkl ({file_size:.1f} MB)\")\n",
        "print(f\"\"\"\n",
        "ì €ì¥ ë‚´ìš©:\n",
        "  - ì „ì²˜ë¦¬ ì „ì²´ ë°ì´í„°: {len(df):,}ê°œ\n",
        "  - ìƒ˜í”Œë§ ë°ì´í„°: {len(df_sampled):,}ê°œ\n",
        "  - í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {TEXT_COLUMN}\n",
        "  - ì¸µí™” ê¸°ì¤€: ë¶„ê¸°(year_quarter)\n",
        "  - ë¶„ê¸° ìˆ˜: {len(quarter_counts)}ê°œ\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "ONvYBAbIqMKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. ì„ë² ë”©"
      ],
      "metadata": {
        "id": "Sk50ZuhltlB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: ì„ë² ë”©\n",
        "\n",
        "import torch\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "OVjlT-cKqMIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "with open('step1_preprocessing_result.pkl', 'rb') as f:\n",
        "    step1_data = pickle.load(f)\n",
        "\n",
        "df_sampled = step1_data['df_sampled']\n",
        "TEXT_COLUMN = step1_data['TEXT_COLUMN']\n",
        "texts = df_sampled[TEXT_COLUMN].tolist()"
      ],
      "metadata": {
        "id": "-zLtec80qMFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ëª¨ë¸ ë¡œë“œ\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model_name = \"monologg/koelectra-base-v3-discriminator\"\n",
        "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
        "model = ElectraModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "S0j9beWaqMDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
        "sample_size = min(1000, len(texts))\n",
        "text_lengths = [len(tokenizer.encode(text)) for text in texts[:sample_size]]\n",
        "avg_length = np.mean(text_lengths)\n",
        "p95_length = np.percentile(text_lengths, 95)\n",
        "\n",
        "print(f\"   í‰ê· : {avg_length:.0f} í† í°\")\n",
        "print(f\"   95%: {p95_length:.0f} í† í°\")\n",
        "\n",
        "max_length = min(int(p95_length), 512)\n",
        "print(f\"   â†’ max_length ì„¤ì •: {max_length}\")"
      ],
      "metadata": {
        "id": "EiaKOCQbqMBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2ì„ë² ë”© í•¨ìˆ˜\n",
        "def get_electra_embeddings(texts, batch_size=16, max_length=128):\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Mean Pooling\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        mean_pooled_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        all_embeddings.extend(mean_pooled_embeddings.cpu().numpy())\n",
        "\n",
        "        del inputs, outputs, last_hidden_states\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return np.array(all_embeddings)\n"
      ],
      "metadata": {
        "id": "M_ZbBXSmqL_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2ì„ë² ë”© ìƒì„±\n",
        "embeddings = get_electra_embeddings(texts, batch_size=16, max_length=max_length)"
      ],
      "metadata": {
        "id": "B1ggFsA4qL80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2í”¼í´ ì €ì¥\n",
        "save_data_step2 = {\n",
        "    'embeddings': embeddings,\n",
        "    'df_sampled': df_sampled,\n",
        "    'texts': texts,\n",
        "    'TEXT_COLUMN': TEXT_COLUMN,\n",
        "    'max_length': max_length,\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open('step2_embeddings_result.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data_step2, f)\n",
        "\n",
        "file_size = os.path.getsize('step2_embeddings_result.pkl') / (1024**2)\n",
        "print(f\"ì €ì¥ ì™„ë£Œ: step2_embeddings_result.pkl ({file_size:.1f} MB)\")\n",
        "print(f\"\"\"\n",
        "ì €ì¥ ë‚´ìš©:\n",
        "  - ì„ë² ë”©: {embeddings.shape}\n",
        "  - ë°ì´í„°í”„ë ˆì„: {len(df_sampled):,}ê°œ\n",
        "  - í…ìŠ¤íŠ¸: {len(texts):,}ê°œ\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "MzfzgJLOqL6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. í´ëŸ¬ìŠ¤í„°ë§"
      ],
      "metadata": {
        "id": "F5hTodhPtgWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#í´ëŸ¬ìŠ¤í„°ë§\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZO7NJx-atEt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "\n",
        "with open('step2_embeddings_result.pkl', 'rb') as f:\n",
        "    step2_data = pickle.load(f)\n",
        "\n",
        "embeddings = step2_data['embeddings']\n",
        "df_sampled = step2_data['df_sampled']\n",
        "texts = step2_data['texts']\n",
        "TEXT_COLUMN = step2_data['TEXT_COLUMN']"
      ],
      "metadata": {
        "id": "GTHc-tswtEqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ìµœì  K ì°¾ê¸°\n",
        "max_k = min(15, len(texts) // 100)\n",
        "k_range = range(2, max_k + 1)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in tqdm(k_range, desc=\"Finding K\"):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    score = silhouette_score(embeddings, clusters)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f\"K={k}: Silhouette Score = {score:.4f}\")"
      ],
      "metadata": {
        "id": "iDkn-yOjtEkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì‹œê°í™”\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(k_range, silhouette_scores, 'bo-', linewidth=2, markersize=10)\n",
        "plt.xlabel('Number of Clusters (K)', fontsize=14)\n",
        "plt.ylabel('Silhouette Score', fontsize=14)\n",
        "plt.title('Optimal K Selection', fontsize=16, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_range)\n",
        "\n",
        "optimal_idx = np.argmax(silhouette_scores)\n",
        "optimal_k = list(k_range)[optimal_idx]\n",
        "optimal_score = silhouette_scores[optimal_idx]\n",
        "\n",
        "plt.axvline(x=optimal_k, color='r', linestyle='--', linewidth=2, label=f'Optimal K={optimal_k}')\n",
        "plt.scatter([optimal_k], [optimal_score], color='red', s=200, zorder=5)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('optimal_k_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nìµœì  K = {optimal_k} (Score: {optimal_score:.4f})\")"
      ],
      "metadata": {
        "id": "KbZ3y3XUt97o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§\n",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n",
        "cluster_labels = kmeans_final.fit_predict(embeddings)\n",
        "\n",
        "df_sampled['cluster'] = cluster_labels"
      ],
      "metadata": {
        "id": "AjpdLVHuxUlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í´ëŸ¬ìŠ¤í„° í†µê³„\n",
        "cluster_counts = df_sampled['cluster'].value_counts().sort_index()\n",
        "for i in range(optimal_k):\n",
        "    count = cluster_counts[i]\n",
        "    percentage = count / len(df_sampled) * 100\n",
        "    bar = \"â–ˆ\" * int(percentage / 2)\n",
        "    print(f\"Cluster {i}  |  {count:5,}ê°œ  ({percentage:5.1f}%)  {bar}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "peXmMTXXt95i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í´ëŸ¬ìŠ¤í„° ìƒ˜í”Œ\n",
        "print(\"\\nğŸ“ í´ëŸ¬ìŠ¤í„°ë³„ ìƒ˜í”Œ (ê° 5ê°œ):\")\n",
        "for i in range(optimal_k):\n",
        "    print(f\"\\n[Cluster {i}]\")\n",
        "    samples = df_sampled[df_sampled['cluster'] == i][TEXT_COLUMN].head(5).tolist()\n",
        "    for idx, sample in enumerate(samples, 1):\n",
        "        print(f\"  {idx}. {sample[:100]}...\")"
      ],
      "metadata": {
        "id": "2c4eOpoSt93P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í”¼í´ ì €ì¥\n",
        "save_data_step3 = {\n",
        "    'embeddings': embeddings,\n",
        "    'df_clustered': df_sampled,\n",
        "    'texts': texts,\n",
        "    'TEXT_COLUMN': TEXT_COLUMN,\n",
        "    'kmeans_model': kmeans_final,\n",
        "    'cluster_labels': cluster_labels,\n",
        "    'optimal_k': optimal_k,\n",
        "    'optimal_score': optimal_score,\n",
        "    'silhouette_scores': silhouette_scores,\n",
        "    'k_range': list(k_range),\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open('step3_clustering_result.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data_step3, f)"
      ],
      "metadata": {
        "id": "xlTj75amt905"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CSV ì €ì¥\n",
        "df_sampled.to_csv('clustering_result.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "file_size = os.path.getsize('step3_clustering_result.pkl') / (1024**2)\n",
        "print(f\"âœ… Step 3 ì €ì¥ ì™„ë£Œ:\")\n",
        "print(f\"   - step3_clustering_result.pkl ({file_size:.1f} MB)\")\n",
        "print(f\"   - clustering_result.csv\")\n",
        "print(f\"\"\"\n",
        "ì €ì¥ ë‚´ìš©:\n",
        "  - í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°: {len(df_sampled):,}ê°œ\n",
        "  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k}ê°œ\n",
        "  - Silhouette Score: {optimal_score:.4f}\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "BKrevH_Xt9yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4. BERTopic ë¶„ì„"
      ],
      "metadata": {
        "id": "j2IhpJyTuTez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "MXkUz9r0uX_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "with open('step3_clustering_result.pkl', 'rb') as f:\n",
        "    step3_data = pickle.load(f)\n",
        "\n",
        "df_clustered = step3_data['df_clustered']\n",
        "TEXT_COLUMN = step3_data['TEXT_COLUMN']\n",
        "optimal_k = step3_data['optimal_k']\n",
        "\n",
        "print(f\"ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ\")\n",
        "print(f\"   ë°ì´í„°: {len(df_clustered):,}ê°œ\")\n",
        "print(f\"   í´ëŸ¬ìŠ¤í„°: {optimal_k}ê°œ\")"
      ],
      "metadata": {
        "id": "ZE06hOt0uaKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í´ëŸ¬ìŠ¤í„°ë³„ BERTopic ìˆ˜í–‰\n",
        "bertopic_results = {}\n",
        "\n",
        "for cluster_id in range(optimal_k):\n",
        "\n",
        "    # í´ëŸ¬ìŠ¤í„° ë°ì´í„° ì¶”ì¶œ\n",
        "    cluster_df = df_clustered[df_clustered['cluster'] == cluster_id]\n",
        "    cluster_texts = cluster_df[TEXT_COLUMN].tolist()\n",
        "\n",
        "    print(f\"ë¬¸ì„œ ìˆ˜: {len(cluster_texts):,}ê°œ\")\n",
        "\n",
        "    if len(cluster_texts) < 10:\n",
        "        print(f\"ë¬¸ì„œê°€ ë„ˆë¬´ ì ì–´ BERTopicì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # BERTopic ëª¨ë¸ ìƒì„±\n",
        "        topic_model = BERTopic(\n",
        "            language='multilingual',\n",
        "            calculate_probabilities=True,\n",
        "            verbose=True,\n",
        "            min_topic_size=min(10, len(cluster_texts) // 10)  # ë™ì  ì¡°ì •\n",
        "        )\n",
        "\n",
        "        # í† í”½ ì¶”ì¶œ\n",
        "        topics, probs = topic_model.fit_transform(cluster_texts)\n",
        "\n",
        "        # í† í”½ ì •ë³´\n",
        "        topic_info = topic_model.get_topic_info()\n",
        "        print(f\"\\në°œê²¬ëœ í† í”½ ìˆ˜: {len(topic_info) - 1}\")  # -1ì€ outlier ì œì™¸\n",
        "        print(\"\\ní† í”½ ì •ë³´:\")\n",
        "        print(topic_info.head(10))\n",
        "\n",
        "        # ê° í† í”½ì˜ ëŒ€í‘œ ë‹¨ì–´\n",
        "        print(f\"\\ní† í”½ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ:\")\n",
        "        for topic_id in topic_model.get_topics():\n",
        "            if topic_id != -1:  # outlier ì œì™¸\n",
        "                words = topic_model.get_topic(topic_id)\n",
        "                top_words = [word for word, _ in words[:10]]\n",
        "                print(f\"  Topic {topic_id}: {', '.join(top_words)}\")\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        bertopic_results[cluster_id] = {\n",
        "            'model': topic_model,\n",
        "            'topics': topics,\n",
        "            'probs': probs,\n",
        "            'topic_info': topic_info,\n",
        "            'cluster_texts': cluster_texts,\n",
        "            'cluster_df': cluster_df,\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        # ê°œë³„ Pickle ì €ì¥\n",
        "        with open(f'step4_bertopic_cluster_{cluster_id}.pkl', 'wb') as f:\n",
        "            pickle.dump(bertopic_results[cluster_id], f)\n",
        "\n",
        "        # í† í”½ ì‹œê°í™” ì €ì¥\n",
        "        try:\n",
        "            fig = topic_model.visualize_topics()\n",
        "            fig.write_html(f'bertopic_cluster_{cluster_id}_topics.html')\n",
        "            print(f\"âœ… ì‹œê°í™” ì €ì¥: bertopic_cluster_{cluster_id}_topics.html\")\n",
        "        except:\n",
        "            print(\"ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨ (ê±´ë„ˆëœ€)\")\n",
        "\n",
        "        print(f\"âœ… Cluster {cluster_id} BERTopic ì™„ë£Œ!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Cluster {cluster_id} BERTopic ì‹¤íŒ¨: {e}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "Ydi5ZfmVuaIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì „ì²´ ê²°ê³¼ ì €ì¥\n",
        "# ì „ì²´ ìš”ì•½ ì •ë³´ë§Œ ì €ì¥ (ëª¨ë¸ ì œì™¸ - ìš©ëŸ‰ ì ˆì•½)\n",
        "summary_results = {}\n",
        "for cluster_id, result in bertopic_results.items():\n",
        "    summary_results[cluster_id] = {\n",
        "        'topic_info': result['topic_info'],\n",
        "        'num_topics': len(result['topic_info']) - 1,\n",
        "        'num_documents': len(result['cluster_texts']),\n",
        "        'timestamp': result['timestamp']\n",
        "    }\n",
        "\n",
        "save_data_step4 = {\n",
        "    'summary_results': summary_results,\n",
        "    'optimal_k': optimal_k,\n",
        "    'total_documents': len(df_clustered),\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open('step4_bertopic_summary.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data_step4, f)\n",
        "\n",
        "print(f\"Step 4 ì €ì¥ ì™„ë£Œ:\")\n",
        "print(f\"   - step4_bertopic_summary.pkl (ì „ì²´ ìš”ì•½)\")\n",
        "for cluster_id in bertopic_results.keys():\n",
        "    file_size = os.path.getsize(f'step4_bertopic_cluster_{cluster_id}.pkl') / (1024**2)\n",
        "    print(f\"   - step4_bertopic_cluster_{cluster_id}.pkl ({file_size:.1f} MB)\")"
      ],
      "metadata": {
        "id": "61ouQCxHuaGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ìµœì¢… ë¶„ì„\n",
        "for cluster_id, result in bertopic_results.items():\n",
        "    topic_count = len(result['topic_info']) - 1\n",
        "    doc_count = len(result['cluster_texts'])\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    print(f\"  - ë¬¸ì„œ ìˆ˜: {doc_count:,}ê°œ\")\n",
        "    print(f\"  - í† í”½ ìˆ˜: {topic_count}ê°œ\")\n",
        "\n",
        "    # ìƒìœ„ 3ê°œ í† í”½\n",
        "    top_topics = result['topic_info'].head(4)  # 0ì€ outlier\n",
        "    if len(top_topics) > 1:\n",
        "        print(f\"  - ì£¼ìš” í† í”½:\")\n",
        "        for idx, row in top_topics.iterrows():\n",
        "            if row['Topic'] != -1:\n",
        "                print(f\"    Topic {row['Topic']}: {row['Count']}ê°œ ë¬¸ì„œ\")\n",
        "\n",
        "print(\"Step 4 ì™„ë£Œ (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ)\")\n",
        "\n",
        "print(f\"\"\"\n",
        "ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ\n",
        "\n",
        "ìƒì„±ëœ íŒŒì¼:\n",
        "Step 1: step1_preprocessing_result.pkl\n",
        "Step 2: step2_embeddings_result.pkl\n",
        "Step 3: step3_clustering_result.pkl\n",
        "Step 4: step4_bertopic_cluster_*.pkl (í´ëŸ¬ìŠ¤í„°ë³„)\n",
        "Step 4: step4_bertopic_summary.pkl (ì „ì²´ ìš”ì•½)\n",
        "CSV: clustering_result.csv\n",
        "ê·¸ë˜í”„: optimal_k_plot.png\n",
        "ì‹œê°í™”: bertopic_cluster_*_topics.html\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "XQ4aKwxfuaEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gApqqOfNuaB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyOEe9sNuZ_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6vKoO_duZ9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDfFgSicuZ7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}